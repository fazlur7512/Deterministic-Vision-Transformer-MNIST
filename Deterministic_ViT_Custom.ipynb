{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB4y0SsdJcCDJ/QQmNrazv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazlur7512/Deterministic-Vision-Transformer-MNIST/blob/main/Deterministic_ViT_Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epILEIYki8qT",
        "outputId": "5c66140d-5ace-45f7-bf06-3c2bdc769434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.7400000095367432\n",
            "Training error 0.785493016242981\n",
            "Percent: [##########] 99.5% Total Training Time:  64.728695539\n",
            " Training Acc    0.7400000095367432\n",
            " Validation Acc  0.7200000286102295\n",
            "------------------------------------\n",
            "Training error    0.785493016242981\n",
            "Validation error 0.7721806168556213\n",
            "Epoch:  2 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.8999999761581421\n",
            "Training error 0.35108134150505066\n",
            "Percent: [##########] 99.5% Total Training Time:  129.2266023799998\n",
            " Training Acc    0.8999999761581421\n",
            " Validation Acc  0.8799999952316284\n",
            "------------------------------------\n",
            "Training error    0.35108134150505066\n",
            "Validation error 0.42985403537750244\n",
            "Epoch:  3 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.9599999785423279\n",
            "Training error 0.18761028349399567\n",
            "Percent: [##########] 99.5% Total Training Time:  192.42880427799992\n",
            " Training Acc    0.9599999785423279\n",
            " Validation Acc  0.9200000166893005\n",
            "------------------------------------\n",
            "Training error    0.18761028349399567\n",
            "Validation error 0.294802725315094\n",
            "Epoch:  4 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   0.9800000190734863\n",
            "Training error 0.12788167595863342\n",
            "Percent: [##########] 99.5% Total Training Time:  257.0839849629999\n",
            " Training Acc    0.9800000190734863\n",
            " Validation Acc  0.9800000190734863\n",
            "------------------------------------\n",
            "Training error    0.12788167595863342\n",
            "Validation error 0.1374027281999588\n",
            "Epoch:  5 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   1.0\n",
            "Training error 0.08342946320772171\n",
            "Percent: [##########] 99.5% Total Training Time:  318.097474445\n",
            " Training Acc    1.0\n",
            " Validation Acc  0.9800000190734863\n",
            "------------------------------------\n",
            "Training error    0.08342946320772171\n",
            "Validation error 0.08848615735769272\n",
            "Epoch:  6 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   1.0\n",
            "Training error 0.06411537528038025\n",
            "Percent: [##########] 99.5% Total Training Time:  379.35860463799986\n",
            " Training Acc    1.0\n",
            " Validation Acc  0.9800000190734863\n",
            "------------------------------------\n",
            "Training error    0.06411537528038025\n",
            "Validation error 0.08265894651412964\n",
            "Epoch:  7 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   1.0\n",
            "Training error 0.06449643522500992\n",
            "Percent: [##########] 99.5% Total Training Time:  442.450610867\n",
            " Training Acc    1.0\n",
            " Validation Acc  1.0\n",
            "------------------------------------\n",
            "Training error    0.06449643522500992\n",
            "Validation error 0.032365549355745316\n",
            "Epoch:  8 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   1.0\n",
            "Training error 0.051838263869285583\n",
            "Percent: [##########] 99.5% Total Training Time:  502.4836759309999\n",
            " Training Acc    1.0\n",
            " Validation Acc  1.0\n",
            "------------------------------------\n",
            "Training error    0.051838263869285583\n",
            "Validation error 0.012719940394163132\n",
            "Epoch:  9 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   1.0\n",
            "Training error 0.07137634605169296\n",
            "Percent: [##########] 99.5% Total Training Time:  563.2711695849998\n",
            " Training Acc    1.0\n",
            " Validation Acc  1.0\n",
            "------------------------------------\n",
            "Training error    0.07137634605169296\n",
            "Validation error 0.04585743322968483\n",
            "Epoch:  10 / 10\n",
            "Percent: [##########] 99.91666666666667% Training Acc   1.0\n",
            "Training error 0.03271495923399925\n",
            "Percent: [##########] 99.5% Total Training Time:  623.6368893699998\n",
            " Training Acc    1.0\n",
            " Validation Acc  1.0\n",
            "------------------------------------\n",
            "Training error    0.03271495923399925\n",
            "Validation error 0.04364728927612305\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    LayerNormalization,\n",
        ")\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "# For multiple devices (GPUs: 4, 5, 6, 7)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,4,5,6,7\"\n",
        "# import imageio\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import time, sys\n",
        "import pickle\n",
        "import timeit\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras import layers\n",
        "#import tensorflow_addons as tfa\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import pandas as pd\n",
        "#import wandb\n",
        "#os.environ[\"WANDB_API_KEY\"] = \"key_code\"\n",
        "\n",
        "import numpy as np\n",
        "#!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from tensorflow.keras import layers\n",
        "#import tensorflow_addons as tfa\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import pandas as pd\n",
        "plt.ioff()\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# update_progress() : Displays or updates a console progress bar\n",
        "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
        "## A value under 0 represents a 'halt'.\n",
        "## A value at 1 or bigger represents 100%\n",
        "def update_progress(progress):\n",
        "    barLength = 10  # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "        status = \"Done...\\r\\n\"\n",
        "    block = int(round(barLength * progress))\n",
        "    text = \"\\rPercent: [{0}] {1}% {2}\".format(\"#\" * block + \"-\" * (barLength - block), progress * 100, status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "class Patches(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size,  1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1,1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads=8):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    if embed_dim % num_heads != 0:\n",
        "      raise ValueError(\n",
        "        f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "      )\n",
        "    self.projection_dim = embed_dim // num_heads\n",
        "    self.query_dense = DDense(embed_dim)\n",
        "    self.key_dense = DDense(embed_dim)\n",
        "    self.value_dense = DDense(embed_dim)\n",
        "    self.combine_heads = DDense(embed_dim)\n",
        "\n",
        "  def attention(self, query, key, value):\n",
        "    score = tf.matmul(query, key, transpose_b=True)\n",
        "    dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    scaled_score = score / tf.math.sqrt(dim_key)\n",
        "    weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "    output = tf.matmul(weights, value)\n",
        "    return output, weights\n",
        "\n",
        "  def separate_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "    query = self.query_dense(inputs)\n",
        "    key = self.key_dense(inputs)\n",
        "    value = self.value_dense(inputs)\n",
        "    query = self.separate_heads(query, batch_size)\n",
        "    key = self.separate_heads(key, batch_size)\n",
        "    value = self.separate_heads(value, batch_size)\n",
        "\n",
        "    attention, weights = self.attention(query, key, value)\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "    concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "    output = self.combine_heads(concat_attention)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(tf.keras.layers.Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "      self.eps = eps\n",
        "      super(LayerNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                   initializer=tf.keras.initializers.Ones(), trainable=True)\n",
        "      self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                  initializer=tf.keras.initializers.Zeros(), trainable=True)\n",
        "      super(LayerNorm, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "      mean = K.mean(x, axis=-1, keepdims=True)\n",
        "      std = K.std(x, axis=-1, keepdims=True)\n",
        "      return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "      return input_shape\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "class DDDense(keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(DDDense, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(name='w', shape=(input_shape[1] * input_shape[2] * input_shape[-1], self.units),\n",
        "                             initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
        "                             trainable=True,\n",
        "                             )\n",
        "\n",
        "  def call(self, input_in):\n",
        "    batch_size = input_in.shape[0]\n",
        "    #flatt = tf.reshape(input_in, [batch_size, -1])  # shape=[batch_size, im_size*im_size*num_channel]\n",
        "    out = tf.matmul(input_in, self.w)\n",
        "    return out\n",
        "\n",
        "class Dsoftmax(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      super(Dsoftmax, self).__init__()\n",
        "\n",
        "    def call(self, input_in):\n",
        "      out = tf.nn.softmax(input_in)\n",
        "      return out\n",
        "\n",
        "class DDense(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        '''\n",
        "        Initialize the instance attributes\n",
        "        '''\n",
        "        super(DDense, self).__init__()\n",
        "        self.units = units\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        '''\n",
        "        Create the state of the layer (weights)\n",
        "        '''\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(name='kernel',\n",
        "                             initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),\n",
        "                             trainable=True)\n",
        "        \n",
        "        # initialize bias\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(name='bias',\n",
        "                             initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
        "                             trainable=True)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        Defines the computation from inputs to outputs\n",
        "        '''\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "class DDropout(keras.layers.Layer):\n",
        "  def __init__(self, drop_prop):\n",
        "    super(DDropout, self).__init__()\n",
        "    self.drop_prop = drop_prop\n",
        "\n",
        "  def call(self, input_in, Training=True):\n",
        "    if Training:\n",
        "      out = tf.nn.dropout(input_in, rate=self.drop_prop)\n",
        "    else:\n",
        "      out = input_in\n",
        "    return out\n",
        "\n",
        "class DGeLU(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(DGeLU, self).__init__()\n",
        "    def call(self, input_in):\n",
        "        out = tf.nn.gelu(input_in)\n",
        "        return out\n",
        "\n",
        "class MLP(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_features, out_features, dropout_rate=0.1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense1 = DDense(hidden_features)\n",
        "        self.dense2 = DDense(out_features)\n",
        "        self.dropout =DDropout(dropout_rate)\n",
        "        self.elu_1 = DGeLU()\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.elu_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "        y = self.dropout(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.mlp = MLP(mlp_dim*2,mlp_dim,dropout)\n",
        "        #self.elu_1 = DGeLU()\n",
        "        self.layernorm1 = LayerNorm(eps=1e-6)\n",
        "        self.layernorm2 = LayerNorm(eps=1e-6)\n",
        "        self.dropout1 = DDropout(dropout)\n",
        "        self.dropout2 = DDropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        inputs_norm = self.layernorm1(inputs)\n",
        "        attn_output = self.att(inputs_norm)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = attn_output + inputs\n",
        "\n",
        "        out1_norm = self.layernorm2(out1)\n",
        "        mlp_output = self.mlp(out1_norm)\n",
        "        mlp_output = self.dropout2(mlp_output, training=training)\n",
        "        return mlp_output + out1\n",
        "\n",
        "\n",
        "class Deterministic_ViT(tf.keras.Model):\n",
        "  def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        mlp_dim,\n",
        "        channels=3,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "      super(Deterministic_ViT, self).__init__()\n",
        "      num_patches = (image_size // patch_size) ** 2\n",
        "      self.patch_dim = channels * patch_size ** 2\n",
        "\n",
        "      self.patch_size = patch_size\n",
        "      self.d_model = d_model\n",
        "      self.num_layers = num_layers\n",
        "\n",
        "      self.rescale = Rescaling(1.0 / 255)\n",
        "      self.pos_emb = self.add_weight(\n",
        "        \"pos_emb\", shape=(1, num_patches + 1, d_model)\n",
        "        )\n",
        "      self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\n",
        "      self.patch_proj = DDense(d_model)\n",
        "      self.enc_layers = [\n",
        "        TransformerBlock(d_model, num_heads, mlp_dim, dropout)\n",
        "        for _ in range(num_layers)\n",
        "        ]\n",
        "        #self.mlp_head = tf.keras.Sequential(\n",
        "        #[\n",
        "        #    LayerNorm(eps=1e-6),\n",
        "        #    DDense(mlp_dim, activation= DGeLU),\n",
        "        #    DDropout(dropout),\n",
        "        #    DDense(num_classes),\n",
        "        #]\n",
        "      self.mlp_head =MLP(mlp_dim, num_classes)\n",
        "  def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1,self.patch_size, self.patch_size, 1],\n",
        "            strides=[1,self.patch_size, self.patch_size, 1],\n",
        "            rates=[1,1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n",
        "        return patches\n",
        "\n",
        "  def call(self, x, training):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = self.rescale(x)\n",
        "        patches = self.extract_patches(x)\n",
        "        x = self.patch_proj(patches)\n",
        "\n",
        "        class_emb = tf.broadcast_to(\n",
        "        self.class_emb, [batch_size, 1, self.d_model]\n",
        "        )\n",
        "        x = tf.concat([class_emb, x], axis=1)\n",
        "        x = x + self.pos_emb\n",
        "\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training)\n",
        "\n",
        "    # First (class token) is used for classification\n",
        "        x = self.mlp_head(x[:, 0])\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main_function(image_size=28,patch_size=7,num_layers=2,num_classes=10,d_model=64,num_heads=2,mlp_dim=64,channels=1,\n",
        "                  dropout=0.1,\n",
        "                  batch_size=50, epochs=10, lr=0.001, lr_end = 0.0001,\n",
        "                  Targeted=False,\n",
        "                Training=True, continue_training=False, saved_model_epochs=10):\n",
        "    #PATH = './saved_models/cnn_epoch_{}/'.format(epochs)\n",
        "    # the data, split between train and test sets\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Scale images to the [0, 1] range\n",
        "    x_train = x_train.astype(\"float32\") / 255\n",
        "    x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "    # Make sure images have shape (28, 28, 1)\n",
        "    x_train = np.expand_dims(x_train, -1)\n",
        "    x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "    trans_model = Deterministic_ViT(image_size=image_size, patch_size=patch_size, num_layers=num_layers,\n",
        "                                  num_classes=num_classes, d_model= d_model, num_heads=num_heads,mlp_dim = mlp_dim,\n",
        "                                  channels= channels)\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "    tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "    num_train_steps = epochs * int(x_train.shape[0] / batch_size)\n",
        "    #    step = min(step, decay_steps)\n",
        "    #    ((initial_learning_rate - end_learning_rate) * (1 - step / decay_steps) ^ (power) ) + end_learning_rate\n",
        "    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=lr,\n",
        "                                                                     decay_steps=num_train_steps,\n",
        "                                                                     end_learning_rate=lr_end, power=2.)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)  # , clipnorm=1.0)\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "\n",
        "    @tf.function  # Make it fast.\n",
        "\n",
        "\n",
        "    def train_on_batch(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            trans_model.trainable = True\n",
        "            out = trans_model(x, training=True)\n",
        "            loss = loss_fn(y, out)\n",
        "            gradients = tape.gradient(loss, trans_model.trainable_weights)\n",
        "\n",
        "            #  gradients = [(tf.where(tf.math.is_nan(grad), tf.constant(1.0e-5, shape=grad.shape), grad)) for grad in gradients]\n",
        "        #  gradients = [(tf.where(tf.math.is_inf(grad), tf.constant(1.0e-5, shape=grad.shape), grad)) for grad in gradients]\n",
        "        optimizer.apply_gradients(zip(gradients, trans_model.trainable_weights))\n",
        "        \n",
        "        return loss, out\n",
        "\n",
        "    @tf.function\n",
        "    def validation_on_batch(x, y):\n",
        "        trans_model.trainable = False\n",
        "        out = trans_model(x, training=False)\n",
        "        total_vloss = loss_fn(y, out)\n",
        "        return total_vloss, out\n",
        "\n",
        "    @tf.function\n",
        "    def test_on_batch(x, y):\n",
        "        trans_model.trainable = False\n",
        "        out = trans_model(x, training=False)\n",
        "        return out\n",
        "    if Training: \n",
        "       # wandb.init(entity = \"dimah\", project=\"DCNN_Cifar10_11layers_epochs_{}_lr_{}_latest\".format(epochs, lr)) \n",
        "        \n",
        "        if continue_training:\n",
        "            saved_model_path = './saved_models/cnn_epoch_{}/'.format(saved_model_epochs)\n",
        "            trans_model.load_weights(saved_model_path + 'Deterministic_cnn_model')\n",
        "        \n",
        "        train_acc = np.zeros(epochs)\n",
        "        valid_acc = np.zeros(epochs)        \n",
        "        train_err = np.zeros(epochs)\n",
        "        valid_err = np.zeros(epochs)                      \n",
        "        start = timeit.default_timer()\n",
        "        for epoch in range(epochs):\n",
        "            print('Epoch: ', epoch + 1, '/', epochs)            \n",
        "            tr_no_steps = 0\n",
        "            va_no_steps = 0\n",
        "            # -------------Training--------------------\n",
        "            acc_training = np.zeros(int(x_train.shape[0] / (batch_size)))\n",
        "            err_training = np.zeros(int(x_train.shape[0] / (batch_size)))            \n",
        "            for step, (x, y) in enumerate(tr_dataset):\n",
        "                update_progress(step / int(x_train.shape[0] / (batch_size)))\n",
        "                loss, out = train_on_batch(x, y)             \n",
        "                err_training[tr_no_steps] = loss.numpy()              \n",
        "                corr = tf.equal(tf.math.argmax(out, axis=-1), tf.math.argmax(y, axis=-1))\n",
        "                accuracy = tf.reduce_mean(tf.cast(corr, tf.float32))                  \n",
        "                acc_training[tr_no_steps] = accuracy.numpy()                                               \n",
        "                tr_no_steps += 1                \n",
        "              \n",
        "            train_acc[epoch] = np.mean(np.amax(acc_training))\n",
        "            train_err[epoch] = np.mean(np.amin(err_training))\n",
        "            print('Training Acc  ', train_acc[epoch])\n",
        "            print('Training error', train_err[epoch])       \n",
        "            # ---------------Validation----------------------  \n",
        "            acc_validation = np.zeros(int(x_test.shape[0] / (batch_size)))\n",
        "            err_validation = np.zeros(int(x_test.shape[0] / (batch_size)))                     \n",
        "            for step, (x, y) in enumerate(val_dataset):\n",
        "                update_progress(step / int(x_test.shape[0] / (batch_size)))\n",
        "                total_vloss, out = validation_on_batch(x, y)                   \n",
        "                err_validation[va_no_steps] = total_vloss.numpy()              \n",
        "                corr = tf.equal(tf.math.argmax(out, axis=-1), tf.math.argmax(y, axis=-1))\n",
        "                va_accuracy = tf.reduce_mean(tf.cast(corr, tf.float32))                 \n",
        "                acc_validation[va_no_steps] = va_accuracy.numpy()                \n",
        "                va_no_steps += 1               \n",
        "            \n",
        "            valid_acc[epoch] = np.mean(np.amax(acc_validation))\n",
        "            valid_err[epoch] = np.mean(np.amin(err_validation))           \n",
        "            stop = timeit.default_timer() \n",
        "            #cnn_model.save_weights(PATH + 'Deterministic_cnn_model')                   \n",
        "##            wandb.log({\"Training Loss\":  train_err[epoch],                        \n",
        "##                       \"Training Accuracy\": train_acc[epoch],                                             \n",
        "##                        \"Validation Loss\": valid_err[epoch],                        \n",
        "##                        \"Validation Accuracy\": valid_acc[epoch],                       \n",
        "##                        'epoch': epoch\n",
        "##                       })             \n",
        "            print('Total Training Time: ', stop - start)\n",
        "            print(' Training Acc   ', train_acc[epoch])            \n",
        "            print(' Validation Acc ', valid_acc[epoch])            \n",
        "            print('------------------------------------')\n",
        "            print('Training error   ', train_err[epoch])            \n",
        "            print('Validation error', valid_err[epoch])                    \n",
        "            # -----------------End Training--------------------------                               \n",
        "           \n",
        "       \n",
        "if __name__ == '__main__':\n",
        "    main_function()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}